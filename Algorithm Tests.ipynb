{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d194bd54-1c5d-45bf-aa36-76ccfcae9bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) PCA after feature engineering\\n2) loading images and metadata into autoencoder\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To do:\n",
    "\"\"\"\n",
    "1) PCA after feature engineering\n",
    "2) figure out how to feature engineer with images (rotations, crops, etc.)\n",
    "3) figure out how to combine image features with metadata\n",
    "4) load images and metadata into autoencoder and get predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a54bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed058f-8682-4c61-815c-1bcb643be5a4",
   "metadata": {},
   "source": [
    "# Loading the Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1ce8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the metadata\n",
    "zip_folder = zipfile.ZipFile('anon-patient-data.zip')\n",
    "skin_cancer_df = pd.read_csv(zip_folder.open('train-metadata.csv'), low_memory=False, \n",
    "                            usecols=[num for num in range(0, 43) if num not in [2, 7]], index_col='isic_id')\n",
    "\n",
    "# Initialize the ResNet model\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Load images and extract features using ResNet50 and batch procesing\n",
    "def extract_features(img):\n",
    "    \"\"\"\n",
    "    Extracts features from a given image\n",
    "    :param img: an image instance in PIL format\n",
    "    :return: the image features\n",
    "    \"\"\"\n",
    "    # Preprocess the image to align with ResNet50 requirements\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array) \n",
    "    \n",
    "    # Extract features\n",
    "    features = resnet_model.predict(img_array)\n",
    "    return features.flatten()\n",
    "\n",
    "def feature_batch_generator(zip_file, img_size=(224, 224), batch_size=32):\n",
    "    \"\"\"\n",
    "    Loads images and extracts feautres with batch processing and a ResNet50 model\n",
    "    :param zip_file: the name of the zip folder that contains the data\n",
    "    :param img_size: the height and width to scale the image to\n",
    "    :param batch_size: the number of images to process in one batch\n",
    "    :return: an array of image features for the batch\n",
    "    \"\"\"\n",
    "    # Indetify all the image files from the zipped folder\n",
    "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "        files = [file for file in z.namelist() if file.startswith('image/') and file.endswith('.jpg')]\n",
    "        # Employ batch processing to load images and extract features\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch_files = files[i:i + batch_size]\n",
    "            features = []\n",
    "            for file in batch_files:\n",
    "                with z.open(file) as img_file:\n",
    "                    img = load_img(BytesIO(img_file.read()), target_size=img_size)\n",
    "                    img_features = extract_features(img)\n",
    "                    features.append(img_features)\n",
    "            # Return the features of the images in the batch\n",
    "            yield np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded8fa8-355f-438d-9396-1768d954cbc8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Metadata by Lesion Type (Cancer vs Non-Cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf24b9-b4c6-4057-892d-28a4d03030e4",
   "metadata": {},
   "source": [
    "### How balanced is the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42dd2928-db81-494a-8868-ab14f0cc2cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 401059 lesions in our dataset, 400666 are not cancerous and 393 are cancerous.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Report the number of cancerous vs non-cancerous lesions in the data\n",
    "not_cancer = skin_cancer_df[skin_cancer_df['target'] == 0]\n",
    "cancer = skin_cancer_df[skin_cancer_df['target'] == 1]\n",
    "print(f'Out of the {len(skin_cancer_df)} lesions in our dataset, {len(not_cancer)} are not cancerous and {len(cancer)} are cancerous.')\n",
    "\n",
    "# Visualize the results in a pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie([len(not_cancer), len(cancer)], labels=['Not Cancer', 'Cancer'], autopct='%1.1f%%')\n",
    "ax.set_title('Proportion of Cancerous vs Non-Cancerous Lesions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afe0bc-a75e-488f-992f-1fb90cd1cb6f",
   "metadata": {},
   "source": [
    "#### The data is heavily imbalanced, with almost all available lesions being non-cancerous. This characteristic of the data is our primary motivator for utilizing anomaly detection rather than binary classification as our method for cancer detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6135be-ced6-4818-8c92-ff25335dd8e6",
   "metadata": {},
   "source": [
    "### Do men and women make up different proportions of cancerous vs non-cancerous lesions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "759724c1-203b-443c-b12f-f24dc9e97e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the frequencies of each sex for cancerous and non-cancerous lesions\n",
    "gender_freqs_cancer = Counter(cancer['sex'])\n",
    "gender_freqs_noncancer = Counter(not_cancer['sex'])\n",
    "\n",
    "# Visualize the frequencies\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].pie([gender_freqs_noncancer['male'], gender_freqs_noncancer['female'], gender_freqs_noncancer[np.nan]],\n",
    "       labels=['male', 'female', 'NA'], autopct='%1.1f%%')\n",
    "ax[0].set_title('Non-Cancerous Patients')\n",
    "ax[1].pie([gender_freqs_cancer['male'], gender_freqs_cancer['female'], gender_freqs_cancer[np.nan]],\n",
    "       labels=['male', 'female', 'NA'], autopct='%1.1f%%')\n",
    "ax[1].set_title('Cancerous Patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36365eb6-8c83-42a8-867f-688ea24bce43",
   "metadata": {},
   "source": [
    "#### Men are more represented in cancerous lesions than non-cancerous lesions, which aligns with the notion that men are more likely to obtain skin cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d5c89-7c82-4499-b262-c3f050b808a0",
   "metadata": {},
   "source": [
    "### Is there a significant difference in the age distribution for cancerous vs non-cancerous patients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578bb0cc-efaa-447e-9e8b-fd6da0a3a75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U test: U-stat = 67731120.5, p-value = 7.030079099323434e-07\n",
      "There is a significant difference in the age distribution between cancerous and non-cancerous patients.\n"
     ]
    }
   ],
   "source": [
    "# Visualize the age distributions\n",
    "plt.hist(cancer['age_approx'], histtype='step', color='red', density=True, label='Cancerous')\n",
    "plt.hist(not_cancer['age_approx'], histtype='step', color='green', density=True, label='Non-Cancerous')\n",
    "plt.legend()\n",
    "plt.xlabel('Age Approximations')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Age Distribution of Cancerous vs Non-Cancerous Patients')\n",
    "plt.show()\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "u_stat, p_value = stats.mannwhitneyu(cancer['age_approx'], not_cancer['age_approx'])\n",
    "\n",
    "# Print the result\n",
    "print(f'Mann-Whitney U test: U-stat = {u_stat}, p-value = {p_value}')\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print('There is a significant difference in the age distribution between cancerous and non-cancerous patients.')\n",
    "else:\n",
    "    print('There is no significant difference in the age distribution between cancerous and non-cancerous patients.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f25af-0b8b-46ad-8ef3-308ca6249774",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "082c1bd2-729e-4ff3-bbf7-1792952575a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>tbp_lv_area_perim_ratio</th>\n",
       "      <th>tbp_lv_color_std_mean</th>\n",
       "      <th>tbp_lv_areaMM2</th>\n",
       "      <th>tbp_lv_eccentricity</th>\n",
       "      <th>tbp_lv_symm_2axis_angle</th>\n",
       "      <th>tbp_lv_norm_border</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.749771</td>\n",
       "      <td>20.730862</td>\n",
       "      <td>1.661645</td>\n",
       "      <td>22.490554</td>\n",
       "      <td>0.716914</td>\n",
       "      <td>87.786260</td>\n",
       "      <td>3.823057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.902299</td>\n",
       "      <td>7.329426</td>\n",
       "      <td>1.450894</td>\n",
       "      <td>27.494953</td>\n",
       "      <td>0.145398</td>\n",
       "      <td>53.357287</td>\n",
       "      <td>1.958035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.010000</td>\n",
       "      <td>11.134446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656784</td>\n",
       "      <td>0.193099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.023068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.400000</td>\n",
       "      <td>15.303800</td>\n",
       "      <td>0.289490</td>\n",
       "      <td>2.852318</td>\n",
       "      <td>0.616804</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.404118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.140000</td>\n",
       "      <td>18.325280</td>\n",
       "      <td>1.489238</td>\n",
       "      <td>12.854190</td>\n",
       "      <td>0.738177</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>3.345044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.870000</td>\n",
       "      <td>24.637490</td>\n",
       "      <td>2.505743</td>\n",
       "      <td>28.767123</td>\n",
       "      <td>0.836360</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>4.924497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.940000</td>\n",
       "      <td>49.869050</td>\n",
       "      <td>8.009495</td>\n",
       "      <td>141.114656</td>\n",
       "      <td>0.960627</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clin_size_long_diam_mm  tbp_lv_area_perim_ratio  tbp_lv_color_std_mean  \\\n",
       "count              393.000000               393.000000             393.000000   \n",
       "mean                 5.749771                20.730862               1.661645   \n",
       "std                  3.902299                 7.329426               1.450894   \n",
       "min                  1.010000                11.134446               0.000000   \n",
       "25%                  2.400000                15.303800               0.289490   \n",
       "50%                  5.140000                18.325280               1.489238   \n",
       "75%                  7.870000                24.637490               2.505743   \n",
       "max                 18.940000                49.869050               8.009495   \n",
       "\n",
       "       tbp_lv_areaMM2  tbp_lv_eccentricity  tbp_lv_symm_2axis_angle  \\\n",
       "count      393.000000           393.000000               393.000000   \n",
       "mean        22.490554             0.716914                87.786260   \n",
       "std         27.494953             0.145398                53.357287   \n",
       "min          0.656784             0.193099                 0.000000   \n",
       "25%          2.852318             0.616804                40.000000   \n",
       "50%         12.854190             0.738177                90.000000   \n",
       "75%         28.767123             0.836360               135.000000   \n",
       "max        141.114656             0.960627               175.000000   \n",
       "\n",
       "       tbp_lv_norm_border  \n",
       "count          393.000000  \n",
       "mean             3.823057  \n",
       "std              1.958035  \n",
       "min              1.023068  \n",
       "25%              2.404118  \n",
       "50%              3.345044  \n",
       "75%              4.924497  \n",
       "max             10.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns to compare summary stats for (choose columns that align with the ABCD factors used for skin cancer detection)\n",
    "use_cols = ['clin_size_long_diam_mm', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_areaMM2', 'tbp_lv_eccentricity', \n",
    "            'tbp_lv_symm_2axis_angle', 'tbp_lv_norm_border']\n",
    "\n",
    "# Present summary statistics for cancerous patients\n",
    "cancer[use_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96dfd094-e95c-4bd2-bec5-fb26588fd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>tbp_lv_area_perim_ratio</th>\n",
       "      <th>tbp_lv_color_std_mean</th>\n",
       "      <th>tbp_lv_areaMM2</th>\n",
       "      <th>tbp_lv_eccentricity</th>\n",
       "      <th>tbp_lv_symm_2axis_angle</th>\n",
       "      <th>tbp_lv_norm_border</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "      <td>400666.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.929043</td>\n",
       "      <td>19.082902</td>\n",
       "      <td>1.069828</td>\n",
       "      <td>8.526291</td>\n",
       "      <td>0.741262</td>\n",
       "      <td>86.330647</td>\n",
       "      <td>3.451159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.738712</td>\n",
       "      <td>5.355337</td>\n",
       "      <td>0.761716</td>\n",
       "      <td>9.635884</td>\n",
       "      <td>0.143854</td>\n",
       "      <td>52.558770</td>\n",
       "      <td>1.724276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.761634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431601</td>\n",
       "      <td>0.027667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.840000</td>\n",
       "      <td>15.426310</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>4.109589</td>\n",
       "      <td>0.656657</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.143715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.370000</td>\n",
       "      <td>17.423380</td>\n",
       "      <td>0.931117</td>\n",
       "      <td>5.685870</td>\n",
       "      <td>0.768237</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2.996717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.380000</td>\n",
       "      <td>21.010776</td>\n",
       "      <td>1.411450</td>\n",
       "      <td>9.101145</td>\n",
       "      <td>0.853182</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>4.357883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.400000</td>\n",
       "      <td>87.205340</td>\n",
       "      <td>9.952932</td>\n",
       "      <td>334.152700</td>\n",
       "      <td>0.974960</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clin_size_long_diam_mm  tbp_lv_area_perim_ratio  tbp_lv_color_std_mean  \\\n",
       "count           400666.000000            400666.000000          400666.000000   \n",
       "mean                 3.929043                19.082902               1.069828   \n",
       "std                  1.738712                 5.355337               0.761716   \n",
       "min                  1.000000                10.761634               0.000000   \n",
       "25%                  2.840000                15.426310               0.574962   \n",
       "50%                  3.370000                17.423380               0.931117   \n",
       "75%                  4.380000                21.010776               1.411450   \n",
       "max                 28.400000                87.205340               9.952932   \n",
       "\n",
       "       tbp_lv_areaMM2  tbp_lv_eccentricity  tbp_lv_symm_2axis_angle  \\\n",
       "count   400666.000000        400666.000000            400666.000000   \n",
       "mean         8.526291             0.741262                86.330647   \n",
       "std          9.635884             0.143854                52.558770   \n",
       "min          0.431601             0.027667                 0.000000   \n",
       "25%          4.109589             0.656657                40.000000   \n",
       "50%          5.685870             0.768237                90.000000   \n",
       "75%          9.101145             0.853182               130.000000   \n",
       "max        334.152700             0.974960               175.000000   \n",
       "\n",
       "       tbp_lv_norm_border  \n",
       "count       400666.000000  \n",
       "mean             3.451159  \n",
       "std              1.724276  \n",
       "min              0.589426  \n",
       "25%              2.143715  \n",
       "50%              2.996717  \n",
       "75%              4.357883  \n",
       "max             10.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Present summary statistics for non-cancerous patients\n",
    "not_cancer[use_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f65d6cb8-bd14-4daa-ad13-380d15ec245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a significant difference in clin_size_long_diam_mm between cancerous and non-cancerous patients.\n",
      "There is a significant difference in tbp_lv_area_perim_ratio between cancerous and non-cancerous patients.\n",
      "There is a significant difference in tbp_lv_color_std_mean between cancerous and non-cancerous patients.\n",
      "There is a significant difference in tbp_lv_areaMM2 between cancerous and non-cancerous patients.\n",
      "There is a significant difference in tbp_lv_eccentricity between cancerous and non-cancerous patients.\n",
      "There is no significant difference in tbp_lv_symm_2axis_angle between cancerous and non-cancerous patients.\n",
      "There is a significant difference in tbp_lv_norm_border between cancerous and non-cancerous patients.\n"
     ]
    }
   ],
   "source": [
    "# Use the Mann-Whitney U test to determine if any of these differences are significant\n",
    "for col in use_cols:\n",
    "    u_stat, p_value = stats.mannwhitneyu(cancer[col], not_cancer[col])\n",
    "    if p_value < 0.05:\n",
    "        print(f'There is a significant difference in {col} between cancerous and non-cancerous patients.')\n",
    "    else:\n",
    "        print(f'There is no significant difference in {col} between cancerous and non-cancerous patients.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65023dfd-22e5-481f-bbb1-31bbd0f6c0e2",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7bdff5d-9df6-4fae-b02d-ac13d4c99725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'age_approx', 'sex', 'anatom_site_general', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cancer_nulls = np.where(not_cancer.isnull().sum() > 0, not_cancer.isnull().sum().index, \"\")\n",
    "not_cancer_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbd39532-5d61-4b6c-b13a-ddc1d05e3906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'age_approx', 'sex', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', ''], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cancer_nulls = np.where(cancer.isnull().sum() > 0, cancer.isnull().sum().index, \"\")\n",
    "not_cancer_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c3c95",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9161d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the categorical (nominal) features\n",
    "categorical_features = skin_cancer_df.select_dtypes(include=['object', 'category', 'string']).columns.tolist()\n",
    "\n",
    "# Impute and encode values in categorical columns\n",
    "updated_features = []\n",
    "for feature in categorical_features:\n",
    "    \n",
    "    # Impute null values in categorical features with the mode\n",
    "    skin_cancer_df[feature] = skin_cancer_df[feature].fillna(skin_cancer_df[feature].mode()[0])\n",
    "    \n",
    "    # Apply one-hot encoding to categorical (nominal) variables\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_feature = encoder.fit_transform(skin_cancer_df[[feature]])\n",
    "    \n",
    "    # Add the encoded columns to the dataframe\n",
    "    encoded_col_names = [f\"{feature}_{cat}\" for cat in encoder.categories_[0]]\n",
    "    encoded_feature_df = pd.DataFrame(encoded_feature, columns=encoded_col_names, index=skin_cancer_df.index)\n",
    "    skin_cancer_df = pd.concat([skin_cancer_df, encoded_feature_df], axis=1)\n",
    "    updated_features += encoded_col_names\n",
    "    \n",
    "# Remove unencoded categorical columns\n",
    "skin_cancer_df = skin_cancer_df.drop(columns=categorical_features)\n",
    "updated_cols = skin_cancer_df.columns\n",
    " \n",
    "# Use KNN to impute null values in the numerical columns\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_array = imputer.fit_transform(skin_cancer_df)\n",
    "skin_cancer_df = pd.DataFrame(imputed_array, columns=updated_cols, index=skin_cancer_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd266b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4951a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing which features to use from feature engineering\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Creates new features to help the model evaluate the ABCD factors used by dermatologists\n",
    "    :param df: a dataframe to add new features to\n",
    "    :return: the input dataframe with updated features\n",
    "    \"\"\"\n",
    "    # A - Asymmetry, Border irregularity/bluriness, and Diameter (skin cancer diameter usually > 6 mm)\n",
    "    df['diameter_ratio'] = df['tbp_lv_minorAxisMM'] / df['clin_size_long_diam_mm']\n",
    "    df['area_irregularity'] = np.abs((np.pi * (df['clin_size_long_diam_mm'] / 2)**2) - (df['tbp_lv_areaMM2'])**(1/2))\n",
    "    df['perimeter_irregularity'] = np.abs((np.pi * df['clin_size_long_diam_mm']) - df['tbp_lv_perimeterMM'])\n",
    "    df['area_perimeter_ratio'] = df['tbp_lv_areaMM2'] / (df['tbp_lv_perimeterMM'] ** 2)\n",
    "    df['large_diameter'] = [1 if val > 5.5 else 0 for val in df['clin_size_long_diam_mm']]\n",
    "    df['perimeter_to_area'] = (df['tbp_lv_perimeterMM']**2) / df['tbp_lv_areaMM2']\n",
    "    df['avg_normalized_irregularity'] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"]) / 2\n",
    "    \n",
    "    # Color (variation)  \n",
    "    df['hc_mean_contrast'] = ((df['tbp_lv_H'] + df['tbp_lv_Hext']) / 2) + ((df['tbp_lv_C'] + df['tbp_lv_Cext']) / 2)\n",
    "    df['tbp_lv_deltaH'] = np.abs(df['tbp_lv_H'] - df['tbp_lv_Hext'])\n",
    "    df['tbp_lv_deltaC'] = np.abs(df['tbp_lv_C'] + df['tbp_lv_Cext'])\n",
    "    df['overall_lab_contrast'] = np.sqrt(df['tbp_lv_deltaL']**2 + df['tbp_lv_deltaA']**2 + df['tbp_lv_deltaB']**2)\n",
    "    df['large_color_variance'] = [1 if val > 4 else 0 for val in df['tbp_lv_color_std_mean']]\n",
    "    df['average_lab_contrast'] = (df['tbp_lv_deltaL'] + df['tbp_lv_deltaA'] + df['tbp_lv_deltaB']) / 3\n",
    "    \n",
    "    # Features to maximize other features\n",
    "    df['lesion_location'] = np.sqrt(df['tbp_lv_x']**2 + df['tbp_lv_y']**2 + df['tbp_lv_z']**2)\n",
    "    df = df.drop(['tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "skin_cancer_enhanced = create_features(skin_cancer_df.copy())\n",
    "\n",
    "# Correlation matrix...\n",
    "\n",
    "# Add column containing image paths\n",
    "skin_cancer_enhanced['image_path'] = pd.Series(skin_cancer_enhanced.index).apply(lambda x:\n",
    "                                    'train-image/image' + str(x) + '.jpg').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe use these features in feature engineering\n",
    "\n",
    "def create_new_features(df):\n",
    "    # Create new features\n",
    "    df[\"color_uniformity\"] = np.where(df[\"tbp_lv_radial_color_std_max\"].values == 0, 0, (df[\"tbp_lv_color_std_mean\"].values /\n",
    "                                                        df[\"tbp_lv_radial_color_std_max\"].values))\n",
    "    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
    "    \n",
    "    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
    "    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d774b4-e842-464c-b9b1-735187206cd1",
   "metadata": {},
   "source": [
    "# PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273c102-4575-4bce-abb6-c1461a9e4f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8878def9",
   "metadata": {},
   "source": [
    "# Isolation Forest - gives worse results than autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7db281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S.S.: 0.17, Estimators: 50, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.50      0.43     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.65    468779\n",
      "weighted avg       0.83      0.80      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 60, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.53      0.44     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.65      0.69      0.66    468779\n",
      "weighted avg       0.84      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 70, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.51      0.43     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.64      0.68      0.66    468779\n",
      "weighted avg       0.83      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 80, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.51      0.43     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.64      0.68      0.66    468779\n",
      "weighted avg       0.83      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 90, f1_score: 0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.50      0.42     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.65    468779\n",
      "weighted avg       0.83      0.80      0.81    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 100, f1_score: 0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.50      0.42     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.65    468779\n",
      "weighted avg       0.83      0.80      0.81    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 110, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.51      0.43     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.66    468779\n",
      "weighted avg       0.83      0.80      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 50, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.38      0.52      0.44     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.64      0.68      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 60, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.39      0.55      0.46     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.67    476792\n",
      "weighted avg       0.83      0.79      0.81    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 70, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.39      0.54      0.45     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 80, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.39      0.54      0.45     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 90, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.38      0.52      0.44     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.64      0.68      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 100, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.38      0.52      0.44     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.64      0.68      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 110, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.39      0.53      0.45     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 50, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.55      0.46     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 60, f1_score: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 70, f1_score: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.56      0.47     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.67    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 80, f1_score: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.56      0.47     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.67    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 90, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.55      0.46     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 100, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.54      0.46     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.68      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 110, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.55      0.46     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "Best Hyperparameters + result: (0.21, 60) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patientsw, w/o feature engineering\n",
    "best = (0, 0, 0, None)\n",
    "for ss in [x*0.01 for x in range(17, 23, 2)]:\n",
    "    # Oversample the minority group to make the data more balanced\n",
    "    smote = SMOTE(sampling_strategy=ss, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_df.drop('target', axis=1), skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Develop and train the Isolation Forest model\n",
    "    c = len(y_resampled[y_resampled == 1]) / len(y_resampled)\n",
    "    for estimators in range(50, 120, 10):\n",
    "        isf = IsolationForest(n_estimators=estimators, contamination=c, random_state=42)\n",
    "        isf.fit(X_train)\n",
    "\n",
    "        # Predict the targets for the test data\n",
    "        preds = isf.predict(X_test)\n",
    "        y_preds = [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "        # Evaluate the models performance on testing data\n",
    "        cr = classification_report(y_resampled, y_preds)\n",
    "        f1_score = float(cr.split()[12])\n",
    "        if f1_score > best[2]:\n",
    "            best = (ss, estimators, f1_score, cr)\n",
    "        print(f'\\nS.S.: {ss}, Estimators: {estimators}, f1_score: {f1_score}')\n",
    "        print(cr)\n",
    "print('Best Hyperparameters + result:', best[:2], '\\n', best[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b4161a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S.S.: 0.17, Estimators: 50, f1_score: 0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.49      0.42     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.67      0.65    468779\n",
      "weighted avg       0.83      0.80      0.81    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 60, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.53      0.44     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.65      0.69      0.66    468779\n",
      "weighted avg       0.84      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 70, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.51      0.43     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.66    468779\n",
      "weighted avg       0.83      0.80      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 80, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.52      0.44     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.64      0.69      0.66    468779\n",
      "weighted avg       0.83      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 90, f1_score: 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.37      0.51      0.43     68113\n",
      "\n",
      "    accuracy                           0.80    468779\n",
      "   macro avg       0.64      0.68      0.66    468779\n",
      "weighted avg       0.83      0.80      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 100, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.53      0.44     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.65      0.69      0.66    468779\n",
      "weighted avg       0.84      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 110, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88    400666\n",
      "         1.0       0.38      0.53      0.45     68113\n",
      "\n",
      "    accuracy                           0.81    468779\n",
      "   macro avg       0.65      0.69      0.66    468779\n",
      "weighted avg       0.84      0.81      0.82    468779\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 50, f1_score: 0.44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.38      0.52      0.44     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.64      0.68      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 60, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.40      0.55      0.46     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.67    476792\n",
      "weighted avg       0.83      0.79      0.81    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 70, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.39      0.53      0.45     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 80, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.39      0.54      0.46     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.81    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 90, f1_score: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87    400666\n",
      "         1.0       0.39      0.53      0.45     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.69      0.66    476792\n",
      "weighted avg       0.82      0.79      0.80    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 100, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.40      0.55      0.46     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.70      0.67    476792\n",
      "weighted avg       0.83      0.79      0.81    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 110, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87    400666\n",
      "         1.0       0.40      0.55      0.46     76126\n",
      "\n",
      "    accuracy                           0.79    476792\n",
      "   macro avg       0.65      0.70      0.67    476792\n",
      "weighted avg       0.83      0.79      0.81    476792\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 50, f1_score: 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.55      0.46     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 60, f1_score: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 70, f1_score: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.56      0.47     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 80, f1_score: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.47     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 90, f1_score: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.40      0.56      0.47     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.65      0.69      0.66    484805\n",
      "weighted avg       0.81      0.78      0.79    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 100, f1_score: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 110, f1_score: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.58      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n",
      "Best Hyperparameters + result: (0.21, 60) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.83      0.86    400666\n",
      "         1.0       0.41      0.57      0.48     84139\n",
      "\n",
      "    accuracy                           0.78    484805\n",
      "   macro avg       0.66      0.70      0.67    484805\n",
      "weighted avg       0.82      0.78      0.80    484805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patients, w feature engineering\n",
    "best = (0, 0, 0, None)\n",
    "for ss in [x*0.01 for x in range(17, 23, 2)]:\n",
    "    # Oversample the minority group to make the data more balanced\n",
    "    smote = SMOTE(sampling_strategy=ss, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_enhanced.drop(['target', 'image_path'], axis=1), \n",
    "                                                  skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Develop and train the Isolation Forest model\n",
    "    c = len(y_resampled[y_resampled == 1]) / len(y_resampled)\n",
    "    for estimators in range(50, 120, 10):\n",
    "        isf = IsolationForest(n_estimators=estimators, contamination=c, random_state=42)\n",
    "        isf.fit(X_train)\n",
    "\n",
    "        # Predict the targets for the test data\n",
    "        preds = isf.predict(X_test)\n",
    "        y_preds = [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "        # Evaluate the models performance on testing data\n",
    "        cr = classification_report(y_resampled, y_preds)\n",
    "        f1_score = float(cr.split()[12])\n",
    "        if f1_score > best[2]:\n",
    "            best = (ss, estimators, f1_score, cr)\n",
    "        print(f'\\nS.S.: {ss}, Estimators: {estimators}, f1_score: {f1_score}')\n",
    "        print(cr)\n",
    "print('Best Hyperparameters + result:', best[:2], '\\n', best[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d218f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S.S.: 0.17, Estimators: 50, f1_score: 0.29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.27      0.32      0.29     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.58    468779\n",
      "weighted avg       0.79      0.78      0.78    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 60, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.27      0.32      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.58    468779\n",
      "weighted avg       0.79      0.78      0.78    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 70, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.28      0.33      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.58    468779\n",
      "weighted avg       0.79      0.78      0.79    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 80, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.27      0.32      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.58    468779\n",
      "weighted avg       0.79      0.78      0.78    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 90, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.28      0.33      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.59    468779\n",
      "weighted avg       0.80      0.78      0.79    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 100, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.28      0.33      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.59    468779\n",
      "weighted avg       0.80      0.78      0.79    468779\n",
      "\n",
      "\n",
      "S.S.: 0.17, Estimators: 110, f1_score: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.85      0.87    400666\n",
      "         1.0       0.28      0.33      0.30     68113\n",
      "\n",
      "    accuracy                           0.78    468779\n",
      "   macro avg       0.58      0.59      0.58    468779\n",
      "weighted avg       0.79      0.78      0.79    468779\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 50, f1_score: 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.85    400666\n",
      "         1.0       0.29      0.34      0.31     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.59      0.58    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 60, f1_score: 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.29      0.34      0.31     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.59      0.58    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 70, f1_score: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.29      0.35      0.32     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.59      0.59    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 80, f1_score: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.29      0.35      0.32     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.59      0.59    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 90, f1_score: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.30      0.35      0.32     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.60      0.59    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 100, f1_score: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.30      0.35      0.32     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.60      0.59    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.19, Estimators: 110, f1_score: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86    400666\n",
      "         1.0       0.29      0.35      0.32     76126\n",
      "\n",
      "    accuracy                           0.76    476792\n",
      "   macro avg       0.58      0.59      0.59    476792\n",
      "weighted avg       0.78      0.76      0.77    476792\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 50, f1_score: 0.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.30      0.36      0.33     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.58      0.59      0.59    484805\n",
      "weighted avg       0.76      0.75      0.75    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 60, f1_score: 0.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.30      0.36      0.33     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.58      0.59      0.59    484805\n",
      "weighted avg       0.76      0.75      0.75    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 70, f1_score: 0.33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.37      0.33     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.58      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 80, f1_score: 0.34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.37      0.34     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.59      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 90, f1_score: 0.34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.38      0.34     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.59      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 100, f1_score: 0.34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.38      0.34     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.59      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n",
      "\n",
      "S.S.: 0.21, Estimators: 110, f1_score: 0.34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.37      0.34     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.59      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n",
      "Best Hyperparameters + result: (0.21, 80) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.84    400666\n",
      "         1.0       0.31      0.37      0.34     84139\n",
      "\n",
      "    accuracy                           0.75    484805\n",
      "   macro avg       0.59      0.60      0.59    484805\n",
      "weighted avg       0.77      0.75      0.76    484805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patients, w feature engineering and feature importance\n",
    "\n",
    "# Sample data\n",
    "X_full = skin_cancer_enhanced.drop(['target', 'image_path'], axis=1)\n",
    "y_full = skin_cancer_df['target']\n",
    "X_train = pd.concat([X_full[y_full == 1][:200], X_full[y_full == 0][:250]], ignore_index=True)\n",
    "y_train = list(y_full[y_full == 1][:200].values) + list(y_full[y_full == 0][:250].values)\n",
    "X_test = pd.concat([X_full[y_full == 1][200:], X_full[y_full == 0][250:450]], ignore_index=True)\n",
    "y_test = list(y_full[y_full == 1][200:].values) + list(y_full[y_full == 0][250:450].values)   \n",
    "\n",
    "# Scale the data between 0 and 1\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Determine important features\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns.to_list())\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns.to_list())\n",
    "c = len([y for y in y_train if y == 1]) / len(y_train)\n",
    "isf = IsolationForest(n_estimators=80, contamination=c, random_state=42)\n",
    "isf.fit(X_train)\n",
    "result = permutation_importance(isf, X_train, y_train, scoring='f1_weighted', n_repeats=10, random_state=42)\n",
    "importance_means = result.importances_mean\n",
    "feature_importances = dict(zip(X_train.columns.to_list(), importance_means))\n",
    "sorted_importances = sorted(feature_importances.items(), key=lambda x:x[1], reverse=True)\n",
    "use_features = [item[0] for item in sorted_importances if item[1] > 0]\n",
    "\n",
    "# Use important features to hyperparameter tune\n",
    "skin_cancer_enhanced_features = skin_cancer_enhanced.copy().drop(use_features, axis=1)\n",
    "\n",
    "best = (0, 0, 0, None)\n",
    "for ss in [x*0.01 for x in range(17, 23, 2)]:\n",
    "    # Oversample the minority group to make the data more balanced\n",
    "    smote = SMOTE(sampling_strategy=ss, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_enhanced_features.drop(['target', 'image_path'], axis=1), \n",
    "                                                  skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Develop and train the Isolation Forest model\n",
    "    c = len(y_resampled[y_resampled == 1]) / len(y_resampled)\n",
    "    for estimators in range(50, 120, 10):\n",
    "        isf = IsolationForest(n_estimators=estimators, contamination=c, random_state=42)\n",
    "        isf.fit(X_train)\n",
    "\n",
    "        # Predict the targets for the test data\n",
    "        preds = isf.predict(X_test)\n",
    "        y_preds = [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "        # Evaluate the models performance 114on testing data\n",
    "        cr = classification_report(y_resampled, y_preds)\n",
    "        f1_score = float(cr.split()[12])\n",
    "        if f1_score > best[2]:\n",
    "            best = (ss, estimators, f1_score, cr)\n",
    "        print(f'\\nS.S.: {ss}, Estimators: {estimators}, f1_score: {f1_score}')\n",
    "        print(cr)\n",
    "print('Best Hyperparameters + result:', best[:2], '\\n', best[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8927345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "036dc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42, criterion = 'gini')\n",
    "dt.fit(X_train, y_train)\n",
    "feature_importances_dt = pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b6731e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tbp_lv_H                                   0.234218\n",
       "clin_size_long_diam_mm                     0.230615\n",
       "tbp_lv_radial_color_std_max                0.084644\n",
       "tbp_lv_minorAxisMM                         0.047223\n",
       "tbp_lv_deltaB                              0.033882\n",
       "tbp_lv_stdLExt                             0.024059\n",
       "size_age_interaction                       0.023143\n",
       "color_uniformity                           0.021359\n",
       "tbp_lv_Bext                                0.021205\n",
       "tbp_lv_L                                   0.019266\n",
       "lesion_size_ratio                          0.018315\n",
       "tbp_lv_x                                   0.016823\n",
       "tbp_lv_deltaLBnorm                         0.016383\n",
       "anatom_site_general_posterior torso        0.016336\n",
       "log_lesion_area                            0.016013\n",
       "tbp_lv_nevi_confidence                     0.015753\n",
       "lesion_color_difference                    0.015429\n",
       "3d_position_distance                       0.014521\n",
       "lesion_severity_index                      0.014469\n",
       "tbp_lv_C                                   0.012000\n",
       "tbp_lv_location_Torso Front Top Half       0.009750\n",
       "overall_color_difference                   0.008710\n",
       "luminance_contrast                         0.008637\n",
       "mean_hue_difference                        0.008531\n",
       "hue_contrast                               0.008432\n",
       "tbp_lv_Hext                                0.008362\n",
       "tbp_lv_areaMM2                             0.008335\n",
       "tbp_lv_z                                   0.008165\n",
       "tbp_lv_norm_color                          0.007891\n",
       "tbp_lv_perimeterMM                         0.006000\n",
       "tbp_lv_A                                   0.006000\n",
       "tbp_lv_eccentricity                        0.004500\n",
       "lesion_visibility_score                    0.004500\n",
       "perimeter_to_area_ratio                    0.003750\n",
       "tbp_lv_location_Left Arm - Lower           0.002087\n",
       "tbp_lv_location_Torso Back Middle Third    0.000694\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_dt[feature_importances_dt > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46692e4a",
   "metadata": {},
   "source": [
    "# Autoencoder - use permutation feauture importances and give it autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28abc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 0.0026 - val_loss: 3.0349e-04\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 2.3854e-04 - val_loss: 1.4470e-04\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0050 - val_loss: 1.6025e-04\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 1.5998e-04 - val_loss: 1.3420e-04\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0014 - val_loss: 1.0507e-04\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 28s 2ms/step - loss: 9.6442e-04 - val_loss: 1.0535e-04\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 1.5342e-04 - val_loss: 8.3812e-05\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 1.3002e-04 - val_loss: 7.7033e-05\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 0.0045 - val_loss: 0.0063\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 0.0027 - val_loss: 7.0433e-05\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 1.1476e-04 - val_loss: 7.2656e-05\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 9.7406e-05 - val_loss: 6.3171e-05\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 1.0930e-04 - val_loss: 7.0697e-05\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0023 - val_loss: 6.4179e-05\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0013 - val_loss: 5.9958e-05\n",
      "Epoch 17/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 0.0010 - val_loss: 7.5961e-05\n",
      "Epoch 18/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 9.0793e-05 - val_loss: 6.1272e-05\n",
      "Epoch 19/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 8.6864e-04 - val_loss: 5.6690e-05\n",
      "Epoch 20/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 1.2879e-04 - val_loss: 5.3476e-05\n",
      "Epoch 21/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 7.8521e-05 - val_loss: 5.1922e-05\n",
      "Epoch 22/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 8.1073e-05 - val_loss: 4.9780e-05\n",
      "Epoch 23/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 24/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 5.3678e-04 - val_loss: 5.7168e-05\n",
      "Epoch 25/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 6.5118e-05 - val_loss: 4.6998e-05\n",
      "Epoch 26/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 1.3546e-04 - val_loss: 5.2445e-05\n",
      "Epoch 27/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 8.3340e-05 - val_loss: 5.5180e-05\n",
      "Epoch 28/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 7.0395e-05 - val_loss: 5.3750e-05\n",
      "Epoch 29/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 0.0031 - val_loss: 5.6707e-05\n",
      "Epoch 30/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 8.3240e-05 - val_loss: 4.6337e-05\n",
      "Epoch 31/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 8.1912e-04 - val_loss: 6.0086e-05\n",
      "Epoch 32/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.7743e-05 - val_loss: 4.3699e-05\n",
      "Epoch 33/100\n",
      "11269/11269 [==============================] - 28s 3ms/step - loss: 6.9966e-05 - val_loss: 5.2756e-05\n",
      "Epoch 34/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 6.5960e-05 - val_loss: 4.6490e-05\n",
      "Epoch 35/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 7.0690e-05 - val_loss: 4.5948e-05\n",
      "Epoch 36/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.6239e-05 - val_loss: 4.6315e-05\n",
      "Epoch 37/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.0263e-05 - val_loss: 4.3083e-05\n",
      "Epoch 38/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.0508e-05 - val_loss: 5.2576e-05\n",
      "Epoch 39/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 7.2434e-05 - val_loss: 4.4708e-05\n",
      "Epoch 40/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 7.0142e-05 - val_loss: 4.2760e-05\n",
      "Epoch 41/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 5.9988e-05 - val_loss: 5.8831e-05\n",
      "Epoch 42/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 7.9174e-05 - val_loss: 4.7703e-05\n",
      "Epoch 43/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 6.7510e-05 - val_loss: 5.1541e-05\n",
      "Epoch 44/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.1037e-05 - val_loss: 4.9499e-05\n",
      "Epoch 45/100\n",
      "11269/11269 [==============================] - 28s 2ms/step - loss: 7.0492e-05 - val_loss: 4.4800e-05\n",
      "Epoch 46/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 6.4613e-05 - val_loss: 4.8879e-05\n",
      "Epoch 47/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 6.3613e-05 - val_loss: 4.3872e-05\n",
      "Epoch 48/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 6.6074e-05 - val_loss: 4.9436e-05\n",
      "Epoch 49/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 5.8609e-05 - val_loss: 4.2116e-05\n",
      "Epoch 50/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 9.8611e-05 - val_loss: 4.8462e-05\n",
      "Epoch 51/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 6.3467e-05 - val_loss: 5.4233e-05\n",
      "Epoch 52/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 6.9342e-05 - val_loss: 4.7564e-05\n",
      "Epoch 53/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 9.6260e-04 - val_loss: 0.0063\n",
      "Epoch 54/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 6.4603e-04 - val_loss: 4.3443e-05\n",
      "Epoch 55/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 5.8229e-05 - val_loss: 3.9004e-05\n",
      "Epoch 56/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 6.0221e-05 - val_loss: 3.9066e-05\n",
      "Epoch 57/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 6.4807e-05 - val_loss: 4.6495e-05\n",
      "Epoch 58/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 6.5888e-05 - val_loss: 6.8500e-05\n",
      "Epoch 59/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 6.5129e-05 - val_loss: 4.1088e-05\n",
      "Epoch 60/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 4.9935e-05 - val_loss: 4.5641e-05\n",
      "Epoch 61/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 7.7562e-05 - val_loss: 4.2922e-05\n",
      "Epoch 62/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 7.6633e-05 - val_loss: 3.8477e-05\n",
      "Epoch 63/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 5.3656e-05 - val_loss: 4.3703e-05\n",
      "Epoch 64/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 5.8478e-05 - val_loss: 4.2339e-05\n",
      "Epoch 65/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 5.4683e-05 - val_loss: 4.7309e-05\n",
      "Epoch 66/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 7.3270e-05 - val_loss: 3.9706e-05\n",
      "Epoch 67/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 5.4532e-05 - val_loss: 3.6314e-05\n",
      "Epoch 68/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 8.7702e-05 - val_loss: 3.7265e-04\n",
      "Epoch 69/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 1.3858e-04 - val_loss: 4.1425e-05\n",
      "Epoch 70/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 5.4091e-05 - val_loss: 4.1548e-05\n",
      "Epoch 71/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 6.6833e-05 - val_loss: 3.7195e-05\n",
      "Epoch 72/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 6.0833e-05 - val_loss: 3.8268e-05\n",
      "Epoch 73/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 5.3818e-05 - val_loss: 3.8178e-05\n",
      "Epoch 74/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 6.7239e-05 - val_loss: 3.5938e-05\n",
      "Epoch 75/100\n",
      "11269/11269 [==============================] - 28s 2ms/step - loss: 5.7076e-05 - val_loss: 3.7128e-05\n",
      "Epoch 76/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 5.4814e-05 - val_loss: 3.5793e-05\n",
      "Epoch 77/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 5.2100e-05 - val_loss: 4.1882e-05\n",
      "Epoch 78/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 5.8805e-05 - val_loss: 3.9911e-05\n",
      "Epoch 79/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 6.2669e-05 - val_loss: 4.5580e-05\n",
      "Epoch 80/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 5.5967e-05 - val_loss: 3.6439e-05\n",
      "Epoch 81/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 6.8612e-05 - val_loss: 6.2266e-05\n",
      "Epoch 82/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 5.3467e-05 - val_loss: 3.7185e-05\n",
      "Epoch 83/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 5.8951e-05 - val_loss: 3.7230e-05\n",
      "Epoch 84/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 4.6056e-05 - val_loss: 3.7606e-05\n",
      "Epoch 85/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 6.4010e-05 - val_loss: 3.5612e-05\n",
      "Epoch 86/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 4.7670e-05 - val_loss: 3.5118e-05\n",
      "Epoch 87/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 5.9805e-05 - val_loss: 3.7925e-05\n",
      "Epoch 88/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 5.7008e-05 - val_loss: 4.3898e-05\n",
      "Epoch 89/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 5.0111e-05 - val_loss: 4.2537e-05\n",
      "Epoch 90/100\n",
      "11269/11269 [==============================] - 28s 2ms/step - loss: 6.5062e-05 - val_loss: 3.3727e-05\n",
      "Epoch 91/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 5.6054e-05 - val_loss: 3.7808e-05\n",
      "Epoch 92/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 5.9957e-05 - val_loss: 3.9530e-05\n",
      "Epoch 93/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 4.9253e-05 - val_loss: 4.3745e-05\n",
      "Epoch 94/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 5.5845e-05 - val_loss: 3.7809e-05\n",
      "Epoch 95/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 5.2506e-05 - val_loss: 3.8229e-05\n",
      "Epoch 96/100\n",
      "11269/11269 [==============================] - 32s 3ms/step - loss: 5.8893e-05 - val_loss: 3.6530e-05\n",
      "Epoch 97/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 5.4986e-05 - val_loss: 4.0384e-05\n",
      "Epoch 98/100\n",
      "11269/11269 [==============================] - 612s 54ms/step - loss: 6.5768e-05 - val_loss: 4.0619e-05\n",
      "Epoch 99/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 6.5160e-05 - val_loss: 3.7100e-05\n",
      "Epoch 100/100\n",
      "11269/11269 [==============================] - 30s 3ms/step - loss: 6.4643e-05 - val_loss: 3.6265e-05\n",
      "13147/13147 [==============================] - 19s 1ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 97, Best Epoch 90 f1 score: 0.73\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99    400666\n",
      "         1.0       0.94      0.59      0.73     20033\n",
      "\n",
      "    accuracy                           0.98    420699\n",
      "   macro avg       0.96      0.80      0.86    420699\n",
      "weighted avg       0.98      0.98      0.98    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 98, Best Epoch 90 f1 score: 0.59\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99    400666\n",
      "         1.0       0.99      0.42      0.59     20033\n",
      "\n",
      "    accuracy                           0.97    420699\n",
      "   macro avg       0.98      0.71      0.79    420699\n",
      "weighted avg       0.97      0.97      0.97    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 99, Best Epoch 90 f1 score: 0.35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       1.00      0.21      0.35     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.98      0.60      0.66    420699\n",
      "weighted avg       0.96      0.96      0.95    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 40s 3ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0047 - val_loss: 0.0034\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 39s 4ms/step - loss: 0.0042 - val_loss: 0.0033\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0040 - val_loss: 0.0031\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0034 - val_loss: 0.0024\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 43s 4ms/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 17/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 18/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 19/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 20/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 21/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 22/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 23/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 24/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 25/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 26/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 27/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 28/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "13147/13147 [==============================] - 19s 1ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 97, Best Epoch 18 f1 score: 0.43\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98    400666\n",
      "         1.0       0.56      0.35      0.43     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.76      0.67      0.70    420699\n",
      "weighted avg       0.95      0.96      0.95    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 98, Best Epoch 18 f1 score: 0.39\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98    400666\n",
      "         1.0       0.65      0.27      0.39     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.81      0.63      0.68    420699\n",
      "weighted avg       0.95      0.96      0.95    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 99, Best Epoch 18 f1 score: 0.28\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       0.79      0.17      0.28     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.88      0.58      0.63    420699\n",
      "weighted avg       0.95      0.96      0.95    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0131 - val_loss: 0.0061\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 43s 4ms/step - loss: 0.0078 - val_loss: 0.0056\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0072 - val_loss: 0.0055\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0070 - val_loss: 0.0054\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0068 - val_loss: 0.0055\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0067 - val_loss: 0.0059\n",
      "Epoch 10/100\n",
      " 5446/11269 [=============>................] - ETA: 19s - loss: 0.0067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b070162e7702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Train the autoencoder using only the non-cancerous patients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_split=0.1,\n\u001b[0m\u001b[0;32m     40\u001b[0m                                  callbacks=[early_stopping])\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         ):\n\u001b[0;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    855\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patients, w feature engineering\n",
    "best = (0, 0, 0, None)\n",
    "# Oversample the minority group to make the data more balanced\n",
    "for ss in range(5, 35, 10):\n",
    "    smote = SMOTE(sampling_strategy=ss*0.01, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_enhanced.drop(['target', 'image_path'], axis=1), \n",
    "                                                  skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Build the autoencoder model - dropout of 5 works best\n",
    "    for d in range(0, 10, 2):\n",
    "        \n",
    "        autoencoder = Sequential([\n",
    "            Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "            Dropout(d*0.1),  \n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(X_train.shape[1], activation='sigmoid')  # Output layer should match input\n",
    "        ])\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Train the autoencoder using only the non-cancerous patients\n",
    "        history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_split=0.1,\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "        # Find the epoch with the lowest validation loss\n",
    "        best_epoch = np.argmin(history.history['val_loss']) + 1  # Add 1 since epochs are 1-indexed\n",
    "        best_val_loss = np.min(history.history['val_loss'])\n",
    "\n",
    "        # Calculate reconstruction error for each sample\n",
    "        reconstructed = autoencoder.predict(X_test)\n",
    "        reconstruction_error = np.mean(np.abs(reconstructed - X_test), axis=1)\n",
    "\n",
    "        # Threshold the reconstruction error to detect anomalies\n",
    "        for thresh in range(97, 100):\n",
    "            threshold = np.percentile(reconstruction_error, thresh)  # Set threshold (e.g., 99th percentile)\n",
    "            predictions_autoencoder = (reconstruction_error > threshold).astype(int)  # 1 = anomaly (cancer), 0 = normal\n",
    "            cr = classification_report(y_resampled, predictions_autoencoder)\n",
    "            f1_score = float(cr.split()[12])\n",
    "            print(f'\\nS.S.: {ss*.01}, Dropout: {d*.1}, Threshold: {thresh}, Best Epoch {best_epoch}',\n",
    "                  f'f1 score: {f1_score}\\n', cr)\n",
    "            if f1_score > best[2]:\n",
    "                best = (ss, d, thresh, best_epoch, f1_score, cr)\n",
    "\n",
    "print('Best:')\n",
    "print(best[:4])\n",
    "print(best[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f127b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 53s 4ms/step - loss: 0.0019 - val_loss: 1.5359e-04\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 0.0063 - val_loss: 0.0186\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 53s 5ms/step - loss: 0.0167 - val_loss: 4.8173e-05\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 9.1448e-05 - val_loss: 7.3300e-05\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 0.0063 - val_loss: 0.0150\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 53s 5ms/step - loss: 0.0034 - val_loss: 5.9904e-05\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 8.8739e-05 - val_loss: 2.5834e-05\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 53s 5ms/step - loss: 0.0123 - val_loss: 3.8658e-05\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 47s 4ms/step - loss: 4.1844e-05 - val_loss: 2.6004e-05\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 54s 5ms/step - loss: 5.4624e-05 - val_loss: 3.2347e-05\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 2.1769e-05 - val_loss: 2.0663e-05\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 0.0048 - val_loss: 4.4392e-05\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 51s 5ms/step - loss: 6.4754e-04 - val_loss: 0.0029\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 51s 5ms/step - loss: 0.0020 - val_loss: 2.7405e-05\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 4.7270e-05 - val_loss: 1.8848e-05\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 49s 4ms/step - loss: 1.6035e-04 - val_loss: 6.0995e-05\n",
      "Epoch 17/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 0.0082 - val_loss: 0.0100\n",
      "Epoch 18/100\n",
      "11269/11269 [==============================] - 49s 4ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 19/100\n",
      "11269/11269 [==============================] - 53s 5ms/step - loss: 0.0031 - val_loss: 2.5060e-05\n",
      "Epoch 20/100\n",
      "11269/11269 [==============================] - 54s 5ms/step - loss: 5.7646e-05 - val_loss: 3.4127e-05\n",
      "Epoch 21/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 3.5270e-05 - val_loss: 1.7764e-05\n",
      "Epoch 22/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 7.8732e-05 - val_loss: 2.6089e-05\n",
      "Epoch 23/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 6.4112e-05 - val_loss: 2.2328e-05\n",
      "Epoch 24/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 2.6922e-05 - val_loss: 2.2897e-05\n",
      "Epoch 25/100\n",
      "11269/11269 [==============================] - 48s 4ms/step - loss: 3.1522e-05 - val_loss: 1.2752e-05\n",
      "Epoch 26/100\n",
      "11269/11269 [==============================] - 57s 5ms/step - loss: 3.0186e-05 - val_loss: 1.2515e-05\n",
      "Epoch 27/100\n",
      "11269/11269 [==============================] - 58s 5ms/step - loss: 5.8633e-05 - val_loss: 2.6613e-04\n",
      "Epoch 28/100\n",
      "11269/11269 [==============================] - 63s 6ms/step - loss: 1.6991e-05 - val_loss: 9.0217e-06\n",
      "Epoch 29/100\n",
      "11269/11269 [==============================] - 57s 5ms/step - loss: 2.2741e-05 - val_loss: 1.1033e-05\n",
      "Epoch 30/100\n",
      "11269/11269 [==============================] - 47s 4ms/step - loss: 2.8785e-05 - val_loss: 1.2178e-05\n",
      "Epoch 31/100\n",
      "11269/11269 [==============================] - 45s 4ms/step - loss: 4.7169e-05 - val_loss: 1.0956e-05\n",
      "Epoch 32/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 4.2744e-05 - val_loss: 1.2410e-05\n",
      "Epoch 33/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 4.8735e-05 - val_loss: 1.6830e-05\n",
      "Epoch 34/100\n",
      "11269/11269 [==============================] - 52s 5ms/step - loss: 2.3863e-05 - val_loss: 1.1997e-05\n",
      "Epoch 35/100\n",
      "11269/11269 [==============================] - 49s 4ms/step - loss: 2.7602e-05 - val_loss: 1.3215e-05\n",
      "Epoch 36/100\n",
      "11269/11269 [==============================] - 54s 5ms/step - loss: 2.1983e-05 - val_loss: 1.0533e-05\n",
      "Epoch 37/100\n",
      "11269/11269 [==============================] - 54s 5ms/step - loss: 4.0125e-05 - val_loss: 1.0506e-05\n",
      "Epoch 38/100\n",
      "11269/11269 [==============================] - 58s 5ms/step - loss: 2.6667e-05 - val_loss: 5.5848e-04\n",
      "13147/13147 [==============================] - 42s 3ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 97, Best Epoch 28 f1 score: 0.77\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99    400666\n",
      "         1.0       1.00      0.63      0.77     20033\n",
      "\n",
      "    accuracy                           0.98    420699\n",
      "   macro avg       0.99      0.81      0.88    420699\n",
      "weighted avg       0.98      0.98      0.98    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 98, Best Epoch 28 f1 score: 0.59\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99    400666\n",
      "         1.0       1.00      0.42      0.59     20033\n",
      "\n",
      "    accuracy                           0.97    420699\n",
      "   macro avg       0.99      0.71      0.79    420699\n",
      "weighted avg       0.97      0.97      0.97    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.0, Threshold: 99, Best Epoch 28 f1 score: 0.35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       1.00      0.21      0.35     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.98      0.61      0.66    420699\n",
      "weighted avg       0.96      0.96      0.95    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 88s 7ms/step - loss: 0.0066 - val_loss: 0.0019\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 83s 7ms/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 82s 7ms/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 78s 7ms/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 77s 7ms/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 76s 7ms/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 78s 7ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 76s 7ms/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 73s 7ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 79s 7ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 9193s 816ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 42s 4ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 28s 3ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "13147/13147 [==============================] - 13s 1000us/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 97, Best Epoch 6 f1 score: 0.64\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99    400666\n",
      "         1.0       0.82      0.52      0.64     20033\n",
      "\n",
      "    accuracy                           0.97    420699\n",
      "   macro avg       0.90      0.76      0.81    420699\n",
      "weighted avg       0.97      0.97      0.97    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 98, Best Epoch 6 f1 score: 0.53\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98    400666\n",
      "         1.0       0.89      0.37      0.53     20033\n",
      "\n",
      "    accuracy                           0.97    420699\n",
      "   macro avg       0.93      0.69      0.75    420699\n",
      "weighted avg       0.97      0.97      0.96    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.2, Threshold: 99, Best Epoch 6 f1 score: 0.33\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       0.96      0.20      0.33     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.96      0.60      0.66    420699\n",
      "weighted avg       0.96      0.96      0.95    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 0.0139 - val_loss: 0.0059\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0078 - val_loss: 0.0047\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 0.0070 - val_loss: 0.0044\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 21s 2ms/step - loss: 0.0067 - val_loss: 0.0043\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 21s 2ms/step - loss: 0.0064 - val_loss: 0.0042\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 0.0063 - val_loss: 0.0039\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0062 - val_loss: 0.0040\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0062 - val_loss: 0.0041\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0061 - val_loss: 0.0040\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0062 - val_loss: 0.0041\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 40s 4ms/step - loss: 0.0061 - val_loss: 0.0040\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0060 - val_loss: 0.0043\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0061 - val_loss: 0.0041\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0061 - val_loss: 0.0041\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 50s 4ms/step - loss: 0.0060 - val_loss: 0.0041\n",
      "13147/13147 [==============================] - 18s 1ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.4, Threshold: 97, Best Epoch 6 f1 score: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98    400666\n",
      "         1.0       0.65      0.41      0.50     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.81      0.70      0.74    420699\n",
      "weighted avg       0.96      0.96      0.96    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.4, Threshold: 98, Best Epoch 6 f1 score: 0.46\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98    400666\n",
      "         1.0       0.78      0.33      0.46     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.87      0.66      0.72    420699\n",
      "weighted avg       0.96      0.96      0.96    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.4, Threshold: 99, Best Epoch 6 f1 score: 0.33\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       0.94      0.20      0.33     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.95      0.60      0.65    420699\n",
      "weighted avg       0.96      0.96      0.95    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0263 - val_loss: 0.0171\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 47s 4ms/step - loss: 0.0193 - val_loss: 0.0150\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 43s 4ms/step - loss: 0.0177 - val_loss: 0.0137\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0162 - val_loss: 0.0125\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0154 - val_loss: 0.0121\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 44s 4ms/step - loss: 0.0151 - val_loss: 0.0123\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0148 - val_loss: 0.0115\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0146 - val_loss: 0.0116\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0145 - val_loss: 0.0115\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0145 - val_loss: 0.0109\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0145 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 37s 3ms/step - loss: 0.0147 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0146 - val_loss: 0.0112\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0147 - val_loss: 0.0115\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0146 - val_loss: 0.0110\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 0.0146 - val_loss: 0.0114\n",
      "Epoch 17/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 0.0145 - val_loss: 0.0120\n",
      "Epoch 18/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0146 - val_loss: 0.0123\n",
      "Epoch 19/100\n",
      "11269/11269 [==============================] - 43s 4ms/step - loss: 0.0150 - val_loss: 0.0110\n",
      "Epoch 20/100\n",
      "11269/11269 [==============================] - 42s 4ms/step - loss: 0.0148 - val_loss: 0.0112\n",
      "13147/13147 [==============================] - 20s 1ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.6000000000000001, Threshold: 97, Best Epoch 10 f1 score: 0.33\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97    400666\n",
      "         1.0       0.43      0.27      0.33     20033\n",
      "\n",
      "    accuracy                           0.95    420699\n",
      "   macro avg       0.70      0.63      0.65    420699\n",
      "weighted avg       0.94      0.95      0.94    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.6000000000000001, Threshold: 98, Best Epoch 10 f1 score: 0.31\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98    400666\n",
      "         1.0       0.52      0.22      0.31     20033\n",
      "\n",
      "    accuracy                           0.95    420699\n",
      "   macro avg       0.74      0.61      0.64    420699\n",
      "weighted avg       0.94      0.95      0.94    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.6000000000000001, Threshold: 99, Best Epoch 10 f1 score: 0.24\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98    400666\n",
      "         1.0       0.69      0.15      0.24     20033\n",
      "\n",
      "    accuracy                           0.96    420699\n",
      "   macro avg       0.83      0.57      0.61    420699\n",
      "weighted avg       0.95      0.96      0.94    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 0.0477 - val_loss: 0.0431\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 0.0420 - val_loss: 0.0462\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 34s 3ms/step - loss: 0.0453 - val_loss: 0.0557\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0478 - val_loss: 0.0498\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 0.0465 - val_loss: 0.0513\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 33s 3ms/step - loss: 0.0464 - val_loss: 0.0576\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0473 - val_loss: 0.0609\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 39s 3ms/step - loss: 0.0476 - val_loss: 0.0629\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 41s 4ms/step - loss: 0.0477 - val_loss: 0.0618\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 38s 3ms/step - loss: 0.0474 - val_loss: 0.0611\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 36s 3ms/step - loss: 0.0480 - val_loss: 0.0596\n",
      "13147/13147 [==============================] - 17s 1ms/step\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.8, Threshold: 97, Best Epoch 1 f1 score: 0.14\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97    400666\n",
      "         1.0       0.19      0.12      0.14     20033\n",
      "\n",
      "    accuracy                           0.93    420699\n",
      "   macro avg       0.57      0.55      0.55    420699\n",
      "weighted avg       0.92      0.93      0.93    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.8, Threshold: 98, Best Epoch 1 f1 score: 0.13\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97    400666\n",
      "         1.0       0.22      0.09      0.13     20033\n",
      "\n",
      "    accuracy                           0.94    420699\n",
      "   macro avg       0.59      0.54      0.55    420699\n",
      "weighted avg       0.92      0.94      0.93    420699\n",
      "\n",
      "\n",
      "S.S.: 0.05, Dropout: 0.8, Threshold: 99, Best Epoch 1 f1 score: 0.1\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.99      0.97    400666\n",
      "         1.0       0.30      0.06      0.10     20033\n",
      "\n",
      "    accuracy                           0.95    420699\n",
      "   macro avg       0.63      0.53      0.54    420699\n",
      "weighted avg       0.92      0.95      0.93    420699\n",
      "\n",
      "Epoch 1/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 2/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 2.9436e-04 - val_loss: 7.0872e-05\n",
      "Epoch 3/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 7.2580e-05 - val_loss: 5.6064e-05\n",
      "Epoch 4/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 3.8598e-04 - val_loss: 0.0023\n",
      "Epoch 5/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 8.2333e-04 - val_loss: 6.7492e-04\n",
      "Epoch 8/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 0.0059 - val_loss: 2.4034e-05\n",
      "Epoch 9/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 4.7157e-05 - val_loss: 2.1953e-05\n",
      "Epoch 10/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 8.4736e-05 - val_loss: 2.1843e-05\n",
      "Epoch 11/100\n",
      "11269/11269 [==============================] - 29s 3ms/step - loss: 3.7250e-05 - val_loss: 1.5437e-05\n",
      "Epoch 12/100\n",
      "11269/11269 [==============================] - 28s 2ms/step - loss: 3.3617e-05 - val_loss: 1.5488e-05\n",
      "Epoch 13/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 0.0094 - val_loss: 9.9998e-05\n",
      "Epoch 14/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 2.9231e-05 - val_loss: 2.4415e-05\n",
      "Epoch 15/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 0.0053 - val_loss: 1.8245e-05\n",
      "Epoch 16/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 5.9325e-05 - val_loss: 1.8423e-05\n",
      "Epoch 17/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 4.9644e-05 - val_loss: 1.8260e-05\n",
      "Epoch 18/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 2.7314e-05 - val_loss: 1.8198e-05\n",
      "Epoch 19/100\n",
      "11269/11269 [==============================] - 28s 3ms/step - loss: 0.0029 - val_loss: 5.9172e-05\n",
      "Epoch 20/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 2.0407e-04 - val_loss: 0.0145\n",
      "Epoch 21/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 4.4701e-05 - val_loss: 1.3648e-05\n",
      "Epoch 22/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 6.2329e-04 - val_loss: 3.0575e-05\n",
      "Epoch 23/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 1.7835e-04 - val_loss: 1.7172e-05\n",
      "Epoch 24/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 3.1537e-05 - val_loss: 1.6257e-05\n",
      "Epoch 25/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 3.7858e-05 - val_loss: 1.4955e-05\n",
      "Epoch 26/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 6.2346e-05 - val_loss: 2.6992e-05\n",
      "Epoch 27/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 4.3608e-05 - val_loss: 2.0345e-05\n",
      "Epoch 28/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 3.0948e-05 - val_loss: 1.5525e-05\n",
      "Epoch 29/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 1.9793e-05 - val_loss: 1.2597e-05\n",
      "Epoch 30/100\n",
      "11269/11269 [==============================] - 25s 2ms/step - loss: 1.3332e-05 - val_loss: 2.0379e-05\n",
      "Epoch 31/100\n",
      "11269/11269 [==============================] - 28s 3ms/step - loss: 2.6562e-04 - val_loss: 0.0031\n",
      "Epoch 32/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 7.8219e-04 - val_loss: 1.4537e-05\n",
      "Epoch 33/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 2.6778e-05 - val_loss: 1.4188e-05\n",
      "Epoch 34/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 5.9972e-04 - val_loss: 1.5045e-05\n",
      "Epoch 35/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 9.8136e-05 - val_loss: 1.2984e-05\n",
      "Epoch 36/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 2.3279e-05 - val_loss: 1.0707e-05\n",
      "Epoch 37/100\n",
      "11269/11269 [==============================] - 24s 2ms/step - loss: 6.6103e-05 - val_loss: 1.0473e-05\n",
      "Epoch 38/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 1.8069e-04 - val_loss: 1.3746e-05\n",
      "Epoch 39/100\n",
      "11269/11269 [==============================] - 23s 2ms/step - loss: 4.0298e-05 - val_loss: 1.1677e-05\n",
      "Epoch 40/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 3.1003e-05 - val_loss: 1.2176e-05\n",
      "Epoch 41/100\n",
      "11269/11269 [==============================] - 27s 2ms/step - loss: 3.3163e-05 - val_loss: 1.1890e-05\n",
      "Epoch 42/100\n",
      "11269/11269 [==============================] - 26s 2ms/step - loss: 2.4638e-05 - val_loss: 1.4148e-05\n",
      "Epoch 43/100\n",
      "11269/11269 [==============================] - 35s 3ms/step - loss: 0.0053 - val_loss: 2.4542e-04\n",
      "Epoch 44/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 2.9251e-05 - val_loss: 1.7215e-05\n",
      "Epoch 45/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 5.4375e-05 - val_loss: 2.2040e-05\n",
      "Epoch 46/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 3.9912e-05 - val_loss: 6.2357e-05\n",
      "Epoch 47/100\n",
      "11269/11269 [==============================] - 31s 3ms/step - loss: 1.5916e-05 - val_loss: 1.3121e-05\n",
      "14399/14399 [==============================] - 24s 2ms/step\n",
      "\n",
      "S.S.: 0.15, Dropout: 0.0, Threshold: 97, Best Epoch 37 f1 score: 0.37\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95    400666\n",
      "         1.0       1.00      0.23      0.37     60099\n",
      "\n",
      "    accuracy                           0.90    460765\n",
      "   macro avg       0.95      0.61      0.66    460765\n",
      "weighted avg       0.91      0.90      0.87    460765\n",
      "\n",
      "\n",
      "S.S.: 0.15, Dropout: 0.0, Threshold: 98, Best Epoch 37 f1 score: 0.27\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94    400666\n",
      "         1.0       1.00      0.15      0.27     60099\n",
      "\n",
      "    accuracy                           0.89    460765\n",
      "   macro avg       0.94      0.58      0.60    460765\n",
      "weighted avg       0.90      0.89      0.85    460765\n",
      "\n",
      "\n",
      "S.S.: 0.15, Dropout: 0.0, Threshold: 99, Best Epoch 37 f1 score: 0.14\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      1.00      0.94    400666\n",
      "         1.0       1.00      0.08      0.14     60099\n",
      "\n",
      "    accuracy                           0.88    460765\n",
      "   macro avg       0.94      0.54      0.54    460765\n",
      "weighted avg       0.89      0.88      0.83    460765\n",
      "\n",
      "Epoch 1/100\n",
      " 6834/11269 [=================>............] - ETA: 14s - loss: 0.0084"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-57cd910d19cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Train the autoencoder using only the non-cancerous patients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_split=0.1,\n\u001b[0m\u001b[0;32m     44\u001b[0m                                  callbacks=[early_stopping])\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         ):\n\u001b[0;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    855\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patients, w feature engineering and feature importances\n",
    "\n",
    "# Use important features to hyperparameter tune\n",
    "skin_cancer_enhanced_features = skin_cancer_enhanced.copy().drop(use_features, axis=1)\n",
    "\n",
    "best = (0, 0, 0, None)\n",
    "# Oversample the minority group to make the data more balanced\n",
    "for ss in range(5, 35, 10):\n",
    "    smote = SMOTE(sampling_strategy=ss*0.01, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_enhanced_features.drop(['target', 'image_path'], axis=1), \n",
    "                                                  skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    \n",
    "    # reduce training size and then split remainder of training into validation and test - for training, just use a \n",
    "    # subset of non-cancerous, then use remainder with cancerous patients to form the validation and test sets\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Build the autoencoder model - dropout of 5 works best\n",
    "    for d in range(0, 10, 2):\n",
    "        \n",
    "        autoencoder = Sequential([\n",
    "            Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "            Dropout(d*0.1),  \n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(X_train.shape[1], activation='sigmoid')  # Output layer should match input\n",
    "        ])\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Train the autoencoder using only the non-cancerous patients\n",
    "        history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_split=0.1,\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "        # Find the epoch with the lowest validation loss\n",
    "        best_epoch = np.argmin(history.history['val_loss']) + 1  # Add 1 since epochs are 1-indexed\n",
    "        best_val_loss = np.min(history.history['val_loss'])\n",
    "\n",
    "        # Calculate reconstruction error for each sample\n",
    "        reconstructed = autoencoder.predict(X_test)\n",
    "        reconstruction_error = np.mean(np.abs(reconstructed - X_test), axis=1)\n",
    "\n",
    "        # Threshold the reconstruction error to detect anomalies\n",
    "        for thresh in range(97, 100):\n",
    "            threshold = np.percentile(reconstruction_error, thresh)  # Set threshold (e.g., 99th percentile)\n",
    "            predictions_autoencoder = (reconstruction_error > threshold).astype(int)  # 1 = anomaly (cancer), 0 = normal\n",
    "            cr = classification_report(y_resampled, predictions_autoencoder, labels=['Not Cancer', 'Cancer'])\n",
    "            f1_score = float(cr.split()[12])\n",
    "            print(f'\\nS.S.: {ss*.01}, Dropout: {d*.1}, Threshold: {thresh}, Best Epoch {best_epoch}',\n",
    "                  f'f1 score: {f1_score}\\n', cr)\n",
    "            if f1_score > best[2]:\n",
    "                best = (ss, d, thresh, best_epoch, f1_score, cr)\n",
    "\n",
    "print('Best:')\n",
    "print(best[:4])\n",
    "print(best[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - training on non-cancerous patients, w/o feature engineering\n",
    "best = (0, 0, 0, None)\n",
    "# Oversample the minority group to make the data more balanced\n",
    "for ss in range(5, 35, 10):\n",
    "    smote = SMOTE(sampling_strategy=ss*0.01, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(skin_cancer_enhanced.drop('target', axis=1), \n",
    "                                                  skin_cancer_df['target'])\n",
    "\n",
    "    # Split the data - training is non-cancerous, test is on all patients to detect anomalies\n",
    "    X_train = X_resampled[y_resampled == 0]\n",
    "    X_test = X_resampled\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Build the autoencoder model - dropout of 5 works best\n",
    "    for d in range(0, 10, 2):\n",
    "        \n",
    "        autoencoder = Sequential([\n",
    "            Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "            Dropout(d*0.1),  \n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(d*0.1),\n",
    "            Dense(X_train.shape[1], activation='sigmoid')  # Output layer should match input\n",
    "        ])\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Train the autoencoder using only the non-cancerous patients\n",
    "        history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_split=0.1,\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "        # Find the epoch with the lowest validation loss\n",
    "        best_epoch = np.argmin(history.history['val_loss']) + 1  # Add 1 since epochs are 1-indexed\n",
    "        best_val_loss = np.min(history.history['val_loss'])\n",
    "\n",
    "        # Calculate reconstruction error for each sample\n",
    "        reconstructed = autoencoder.predict(X_test)\n",
    "        reconstruction_error = np.mean(np.abs(reconstructed - X_test), axis=1)\n",
    "\n",
    "        # Threshold the reconstruction error to detect anomalies\n",
    "        for thresh in range(97, 100):\n",
    "            threshold = np.percentile(reconstruction_error, thresh)  # Set threshold (e.g., 99th percentile)\n",
    "            predictions_autoencoder = (reconstruction_error > threshold).astype(int)  # 1 = anomaly (cancer), 0 = normal\n",
    "            cr = classification_report(y_resampled, predictions_autoencoder)\n",
    "            f1_score = float(cr.split()[12])\n",
    "            print(f'\\nS.S.: {ss*.01}, Dropout: {d*.1}, Threshold: {thresh}, Best Epoch {best_epoch}',\n",
    "                  f'f1 score: {f1_score}\\n', cr)\n",
    "            if f1_score > best[2]:\n",
    "                best = (ss, d, thresh, best_epoch, f1_score, cr)\n",
    "\n",
    "print('Best:')\n",
    "print(best[:4])\n",
    "print(best[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
